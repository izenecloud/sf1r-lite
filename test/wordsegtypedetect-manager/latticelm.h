/*
* Copyright 2010, Graham Neubig
* 
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
* 
*     http://www.apache.org/licenses/LICENSE-2.0
* 
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

#ifndef LATTICELM_H
#define LATTICELM_H

#include "singlesample.h"
#include "pylm.h"
#include "lexfst.h"
#include "pylmfst.h"
#include "weighted-mapper.h"
#include "sampgen.h"
#include <fst/compose.h>
#include <list>
#include <fst/prune.h>
#include <fst/arcsort.h>
#include <boost/date_time/posix_time/posix_time.hpp>

#define MAX_WORD_LEN 1e3
//#define TimeDebug
using namespace std;
using namespace pylm;
using namespace fst;
boost::posix_time::time_duration graphtime,viterbetime ,parseSampletime,traintime,sampleParameterstime,iterateSamplestime,likelihoodime,removeSampletime,allSampletime,singleSampletime,wholetime,composetime;

namespace latticelm {
string stop[]={"与","的","(",")","（","）","-",":","【","】","+","之","《","》",".","；","？","！","!","《","》","\"","#","$","&","'","/","\\","]","[","|","/",";","【","●","，","、","*"};
vector<string> stopWord(stop,stop+36);
class LatticeLM {

private:

    // type definitions
    typedef PhiMatcher< Matcher<Fst<StdArc> > > PM;

    // iteration parameters
    unsigned numBurnIn_; // the number of burn in (20)
    unsigned numAnnealSteps_; // the number of annealing steps (5)
    unsigned annealStepLength_; // the number of steps to anneal for (3)
    unsigned numSamples_; // the number of samples to take (100)
    unsigned sampleRate_; // the number of iterations between samples (1)
    unsigned trimRate_; // the number of iterations between trims (1)

    // training parameters
    double pruneThreshold_; // prune paths this far away (0, no pruning)
    double amScale_; // how much to scale the acoustic model (0.2)
    unsigned knownN_; // the n-gram size of the known word LM (3)
    unsigned unkN_; // the n-gram size of the unk LM (3)

    // input parameters
    const char* inputFileList_; // the list of files to be input
    vector< string > inputFiles_; // the files to be used as input

    static const unsigned INPUT_FST = 0;  // use FST input
    static const unsigned INPUT_TEXT = 1; // use text input
    unsigned inputType_;           // the type of input to use

    bool cacheInput_;
    vector< Fst<StdArc> * > inputFsts_; // the FSTs, if cached
    const char* symbolFile_; // a file containing the symbols

    // output parameters
    string prefix_; // the prefix of the output
    string separator_; // the character to use to separate the characters

    // training variables
    vector<unsigned> mySamples_; // which samples to use
    vector< vector<WordId> > histories_;
    unsigned unkSymbolSize_;
    double annealLevel_;

    // data structure
    LexFst<WordId, CharId> * lexFst_;
    PyLM<WordId> * knownLm_;
    PyLM<CharId> * unkLm_;
    vector<LMProb> unkBases_;

    // information variables
    double latticeLikelihood_; // the likelihood of the acoustic model
    double knownLikelihood_; // the likelihood of words generated by the LM
    double unkLikelihood_; // the likelihood of words generated by the unknown model
    PylmFst<WordId,CharId>* pylmFst;
    //save and load
    string trainfile_;
    bool load_;
    ofstream trainofs;
    ifstream trainifs;
public:
    

    LatticeLM() : numBurnIn_(1), numAnnealSteps_(5), annealStepLength_(3),
        numSamples_(100), sampleRate_(1), trimRate_(1),
        pruneThreshold_(0), amScale_(0.2), knownN_(3), unkN_(3),
        inputFileList_(0), inputType_(INPUT_TEXT),
        cacheInput_(false), symbolFile_(0),
        prefix_(), separator_(), unkSymbolSize_(0), annealLevel_(0),
        lexFst_(0), knownLm_(0), unkLm_(0), unkBases_(),load_(false)
    {

    }

    
    void save(ofstream& ofs)
    {
         ofs|numBurnIn_|numAnnealSteps_|annealStepLength_|annealStepLength_|numSamples_|sampleRate_|trimRate_|pruneThreshold_|amScale_|knownN_|unkN_;
         ofs|unkSymbolSize_|annealLevel_;
         //cout<<"prefix_"<<annealLevel_<<unkSymbolSize_<<endl;
         saveString(ofs,prefix_);
         //cout<<"prefix_"<<prefix_<<endl;
         saveString(ofs,separator_);
         knownLm_->save(ofs);
         unkLm_->save(ofs);
    }

    void load(ifstream& ifs)
    {
         ifs|numBurnIn_|numAnnealSteps_|annealStepLength_|annealStepLength_|numSamples_|sampleRate_|trimRate_|pruneThreshold_|amScale_|knownN_|unkN_;
         ifs|unkSymbolSize_|annealLevel_;
         //cout<<"prefix_"<<annealLevel_<<unkSymbolSize_<<endl;
         loadString(ifs,prefix_);
         //cout<<"prefix_"<<prefix_<<endl;
         loadString(ifs,separator_);
         knownLm_ = new PyLM<WordId>(knownN_);
         unkLm_ = new PyLM<CharId>(unkN_);
         knownLm_->load(ifs);
         unkLm_->load(ifs);
        
    }


    ~LatticeLM() {
        if(lexFst_)  delete lexFst_;
        if(knownLm_) delete knownLm_;
        if(unkLm_)   delete unkLm_;
    }

    void dieOnHelp(const char* err) {
        cerr << "---latticelm v. 0.2 (9/21/2010)---" << endl
<< " A tool for learning a language model and a word dictionary" << endl
<< " from lattices (or text) using Pitman-Yor language models and" << endl
<< " Weighted Finite State Transducers" << endl
<< "  By Graham Neubig" << endl << endl
<< "Usage: latticelm -prefix out/ input.txt" << endl
<< "Options:" << endl
<< "  -burnin:       The number of iteration to execute as burn-in (20)" << endl
<< "  -annealsteps:  The number of annealing steps to perform (3)" << endl
<< "                 See Goldwater+ 2009 for details on annealing." << endl
<< "  -anneallength: The length of each annealing step in iterations (5)" << endl
<< "  -samps:        The number of samples to take (100)" << endl
<< "  -samprate:     The frequency (in iterations) at which to take samples (1)" << endl
<< "  -knownn:       The n-gram length of the language model (3)" << endl
<< "  -unkn:         The n-gram length of the spelling model (3)" << endl
<< "  -prune:        If this is activated, paths that are worse than the" << endl
<< "                 best path by at least this much will be pruned." << endl
<< "  -input:        The type of input (text/fst, default text)." << endl
<< "  -filelist:     A list of input files, one file per line." << endl
<< "                 For fst input, files must be in OpenFST binary "<<endl
<< "                 format, tropical semiring. Text files consist of one" << endl
<< "                 sentence per line." << endl
<< "  -symbolfile:   The symbol file for the WFSTs, not used for text input." << endl
<< "  -prefix:       The prefix under which to print all output." << endl
<< "  -separator:    The string to use to separate 'characters'." << endl
<< "  -cacheinput:   For WFST input, cache the WFSTs in memory (otherwise" << endl
<< "                 they will be loaded from disk every iteration)." << endl
<< "  -load:         Add by qian wang,save the train result" << endl;
        if(err)
            cerr << endl << "Error: " << err << endl;
        exit(1);
    }

    CharId findId(const string & str, unordered_map<string,CharId> & idHash, vector<string> & idList) {
        unordered_map<string,CharId>::iterator it = idHash.find(str);
        if(it == idHash.end()) {
            idHash.insert(pair<string,CharId>(str,idHash.size()));
            idList.push_back("x"+str);
            return idHash.size()-1;
        }
        return it->second;
    }
    vector<string> loadText() {
        unordered_map<string,CharId> idHash;
        vector<string> idList;
        int state;
        // NOTE: <unk> and </unk> are included in the output vocab,
        //       but not the input. They must not occur in the input
        //       file.
        findId("<eps>",idHash,idList);
        findId("<phi>",idHash,idList);
        idList.push_back("x<unk>");
        idList.push_back("x</unk>");
        for(unsigned i = 0; i < inputFiles_.size(); i++) {
            ifstream in(inputFiles_[i].c_str());
            string line, str;
            while(getline(in,line)) {
                istringstream iss(line);
                StdVectorFst * fst = new StdVectorFst;
                fst->AddState();
                fst->SetStart(0);
                for(state = 0; iss >> str; state++) {
                    CharId lab = findId(str,idHash,idList);
                    //cout<<str<<lab<<endl;
                    fst->AddState();
                    fst->AddArc(state,StdArc(lab,lab,0,state+1));
                }
                //sleep(1);
                fst->SetFinal(state,0);
                if(state == 0) {
                    cerr << "Empty line found in "<<inputFiles_[i]<<endl;
                    cerr << "Please ensure that each line in the training file contains at least one symbol."<<endl;
                    exit(1);
                }
                inputFsts_.push_back(fst);
            }
             
        }
        idList.push_back("w<s>");
        // idList.push_back("</s>");
        return idList;
    }
    void loadFile(const string& trainfile,const string& symbolefile)
    {
        //CharId argPos = 1;
        ostringstream err;
        string prefix_ = "prefix";
        trainfile_=trainfile;
        load_ = true;
        //inputFileList_=symbolefile.c_str();
        if(inputType_ == INPUT_TEXT) cacheInput_ = true;
 
        // load the input files, either from the list or not
        inputFiles_.push_back(symbolefile);
        knownLm_ = new PyLM<WordId>(knownN_);
        unkLm_ = new PyLM<CharId>(unkN_);
        if(load_ )
        {
             trainifs.open(trainfile_.c_str(),ios::in);
             //cout<<trainfile_<<endl;
             if(!trainifs) cout<< "count't load file    "<<trainfile_<<endl;
             load(trainifs);
             trainifs.close();
        }
        lexFst_ = new LexFst<WordId,CharId>(*knownLm_, *unkLm_, unkSymbolSize_);
        lexFst_->setSeparator(separator_);
        // sanity check for the FST input
        if(inputType_ == INPUT_FST) {
            inputFsts_.resize(inputFiles_.size(),0);
            if(!symbolFile_)
                dieOnHelp("No symbol file was set");
            lexFst_->load(symbolFile_);
        }
        // load the text input
        else { 
            lexFst_->setPermSymbols(loadText());
            lexFst_->initializeArcs();
        }
        histories_.resize(inputFsts_.size());

        // load the symbols for the lexicon FST
        unkSymbolSize_ = lexFst_->getNumChars();
        unkBases_.resize(MAX_WORD_LEN,1.0/unkSymbolSize_);
        //cerr << "Loaded " << unkSymbolSize_ << " symbols";
        //if(symbolFile_) cerr << " from " << symbolFile_;
        //cerr << endl;

        // load the LMs
        
        

        const vector<string> & symbols = lexFst_->getSymbols();
        //const vector< LMProb > wordBases = calculateWordBases();
        computeLm(unkLm_,&symbols[2],1.0/lexFst_->unkVocabSize_); 
        //computeLm(knownLm_,&symbols[2+unkSymbolSize_],&wordBases[0]);
        // perform sanity check
        pylmFst=new  PylmFst<WordId,CharId>(*knownLm_, *unkLm_, unkSymbolSize_);
        if(inputFiles_.size() == 0)
            dieOnHelp("No input files specified");
        else if(prefix_.length() == 0)
            dieOnHelp("No output prefix was specified");
    }
    void loadProperties(int argc, char** argv) {
        // read the arguments
        CharId argPos = 1;
        ostringstream err;
        for( ; argPos < argc && argv[argPos][0] == '-'; argPos++) {
            if(!strcmp(argv[argPos],"-burnin")) numBurnIn_ = atoi(argv[++argPos]);
            else if(!strcmp(argv[argPos],"-annealsteps")) numAnnealSteps_ = atoi(argv[++argPos]);
            else if(!strcmp(argv[argPos],"-anneallength")) annealStepLength_ = atoi(argv[++argPos]);
            else if(!strcmp(argv[argPos],"-samps")) numSamples_ = atoi(argv[++argPos]);
            else if(!strcmp(argv[argPos],"-samprate")) sampleRate_ = atoi(argv[++argPos]);
            else if(!strcmp(argv[argPos],"-knownn")) knownN_ = atoi(argv[++argPos]);
            else if(!strcmp(argv[argPos],"-unkn")) unkN_ = atoi(argv[++argPos]);
            else if(!strcmp(argv[argPos],"-prune")) pruneThreshold_ = atof(argv[++argPos]);
            else if(!strcmp(argv[argPos],"-filelist")) inputFileList_ = argv[++argPos];
            else if(!strcmp(argv[argPos],"-input")) {
                ++argPos;
                if(!strcmp("fst",argv[argPos]))  inputType_ = INPUT_FST;
                else if(!strcmp("text",argv[argPos])) inputType_ = INPUT_TEXT;
                else {
                    err << "Bad input type '"<<argv[argPos]<<"'";
                    dieOnHelp(err.str().c_str());
                }
            }
            else if(!strcmp(argv[argPos],"-symbolfile")) symbolFile_ = argv[++argPos];
            else if(!strcmp(argv[argPos],"-trainfile")) trainfile_ = argv[++argPos];
            else if(!strcmp(argv[argPos],"-load")) load_ = true;
            else if(!strcmp(argv[argPos],"-prefix"))     prefix_ = argv[++argPos];
            else if(!strcmp(argv[argPos],"-separator"))  separator_ = argv[++argPos];
            else if(!strcmp(argv[argPos],"-cacheinput")) cacheInput_ = true;
            else {
                err << "Illegal option: " << argv[argPos];
                dieOnHelp(err.str().c_str());
            }
        }
        if(inputType_ == INPUT_TEXT) cacheInput_ = true;
 
        // load the input files, either from the list or not
        if(inputFileList_) {
            ifstream files(inputFileList_);
            if(!files) {
                err << "Couldn't find the file list: "<<inputFileList_ <<endl;
                dieOnHelp(err.str().c_str());
            }
            string buff;
            while(getline(files,buff)) {
                inputFiles_.push_back(buff);
                ifstream checkFile(buff.c_str());
                if(!checkFile) {
                    err << "Couldn't find input file: '" << buff << "'" << endl;
                    dieOnHelp(err.str().c_str());
                }
                checkFile.close();
            }
            files.close();
        } else {
            for( ; argPos < argc; argPos++) {
                inputFiles_.push_back(string(argv[argPos]));
                ifstream checkFile(argv[argPos]);
                if(!checkFile) {
                    err << "Couldn't find input file: " << argv[argPos] <<endl;
                    dieOnHelp(err.str().c_str());
                }
                checkFile.close();
            }
        }
        knownLm_ = new PyLM<WordId>(knownN_);
        unkLm_ = new PyLM<CharId>(unkN_);
        if(load_ )
        {
             trainifs.open(trainfile_.c_str(),ios::in);
             //cout<<trainfile_<<endl;
             if(!trainifs) cout<< "count't load file    "<<trainfile_<<endl;
             load(trainifs);
             trainifs.close();
        }
        lexFst_ = new LexFst<WordId,CharId>(*knownLm_, *unkLm_, unkSymbolSize_);
        lexFst_->setSeparator(separator_);
        // sanity check for the FST input
        if(inputType_ == INPUT_FST) {
            inputFsts_.resize(inputFiles_.size(),0);
            if(!symbolFile_)
                dieOnHelp("No symbol file was set");
            lexFst_->load(symbolFile_);
        }
        // load the text input
        else { 
            lexFst_->setPermSymbols(loadText());
            lexFst_->initializeArcs();
        }
        histories_.resize(inputFsts_.size());

        // load the symbols for the lexicon FST
        unkSymbolSize_ = lexFst_->getNumChars();
        unkBases_.resize(MAX_WORD_LEN,1.0/unkSymbolSize_);
        cerr << "Loaded " << unkSymbolSize_ << " symbols";
        if(symbolFile_) cerr << " from " << symbolFile_;
        cerr << endl;

        // load the LMs
        
        
        // perform sanity check
        if(inputFiles_.size() == 0)
            dieOnHelp("No input files specified");
        else if(prefix_.length() == 0)
            dieOnHelp("No output prefix was specified");

    }

    // train the model on all the data
    void train() {
        if(load_){return;}
        // initialize mySamples to 0,1,2,3..n-1
        mySamples_ = vector<unsigned>(inputFsts_.size());
        for(unsigned i = 0; i < mySamples_.size(); i++)
            mySamples_[i] = i;

        // iterate
        for(unsigned iter = 0; iter <= numSamples_; iter++) {
            unkLikelihood_ = 0; knownLikelihood_ = 0; latticeLikelihood_ = 0;

            annealLevel_ = (int)(iter+annealStepLength_-1)/annealStepLength_;
            if(annealLevel_ != 0)
                annealLevel_ = 1.0/max(1.0,numAnnealSteps_-annealLevel_);
            if(iter>20)
                annealLevel_=10000;
#ifdef TimeDebug
        boost::posix_time::ptime time_now = boost::posix_time::microsec_clock::local_time();
#endif            
            // iterate
            iterateSamples(annealLevel_);
#ifdef TimeDebug
        boost::posix_time::ptime time_then = boost::posix_time::microsec_clock::local_time();
        iterateSamplestime =iterateSamplestime+( time_then - time_now);//
#endif
#ifdef TimeDebug
        time_now = boost::posix_time::microsec_clock::local_time();
#endif
            // sample the model parameters and print status
            sampleParameters();

#ifdef TimeDebug
        time_then = boost::posix_time::microsec_clock::local_time();
        sampleParameterstime =sampleParameterstime+( time_then - time_now);//
#endif
            printIterationStatus(iter);
       
            // trim down the size if necessary
            if(iter%trimRate_ == 0)
                trimModels();

            // print a sample if necessary
            if(iter >= numBurnIn_ && (iter-numBurnIn_)%sampleRate_==0) {
                cerr << " Printing sample for iteration "<<iter<<endl;
                printSample(iter);
            }
            //TimeShow();
            if(!trainfile_.empty())
            {
                 trainofs.open(trainfile_.c_str(),ios::out);
                 save(trainofs);
                 trainofs.close();
            }
        }

    }

    // trim the models, removing unneeded vocabulary
    void trimModels() {
        // trim the language model
        vector<WordId> trimmedIds = knownLm_->trim(true);
        unkLm_->trim(false);
        // trim the lexicon
        const vector< vector<CharId> > & knownWords = lexFst_->getWords();
        LexFst<WordId,CharId> * nextLex = new LexFst<WordId,CharId>(*knownLm_, *unkLm_, unkSymbolSize_);
        nextLex->setSeparator(separator_);
        nextLex->setPermSymbols(lexFst_->getPermSymbols());
        nextLex->initializeArcs();
        for(unsigned i = 0; i < knownWords.size(); i++) {
            if(trimmedIds[i] != -1)
                nextLex->addWord(knownWords[i]);
        }
        // const vector< string > & knownSymbols = lexFst_->getSymbols();
        // const vector< string > & newSymbols = nextLex->getSymbols();
        // re-map the history
        for(unsigned i = 0; i < histories_.size(); i++)
            for(unsigned j = 0; j < histories_[i].size(); j++)
                histories_[i][j] = trimmedIds[histories_[i][j]];
        delete lexFst_;
        lexFst_ = nextLex;
    }

    // print the status of the current iteration
    void printIterationStatus(unsigned int iter, ostream & out = cerr) {
        
        out << "Finished iteration " << iter << " (Anneal="<<annealLevel_<<"), LM="<< (knownLikelihood_+unkLikelihood_) 
            << " (w=" << knownLikelihood_ << ", u="<<unkLikelihood_<<"), Lattice=" << latticeLikelihood_ << endl
             << " Vocabulary: w=" << knownLm_->getVocabSize() <<", u="<<unkLm_->getVocabSize() << endl
             << " LM size: w=" << knownLm_->size() <<", u="<<unkLm_->size() << endl;
        for(int i = 0; i < knownLm_->getN(); i++)
            out << " WLM " << (i+1) << "-gram, s="<<knownLm_->getStrength(i)<<", d="<<knownLm_->getDiscount(i)<<endl;
        for(int i = 0; i < unkLm_->getN(); i++)
            out << " CLM " << (i+1) << "-gram, s="<<unkLm_->getStrength(i)<<", d="<<unkLm_->getDiscount(i)<<endl;
    }
    
    // sample the model parameters
    void sampleParameters() {
        knownLm_->sampleParameters();
        unkLm_->sampleParameters();
    }

    // print a single sample to the appropriate file
    void printSample(int iter = -1) {
        const vector<string> & symbols = lexFst_->getSymbols();
        // const vector< vector<CharId> > & words = lexFst_->getWords();
        writeLm(unkLm_,&symbols[2],&unkBases_[0],prefix_+"ulm",iter);
        const vector< LMProb > wordBases = calculateWordBases();
        writeLm(knownLm_,&symbols[2+unkSymbolSize_],&wordBases[0],prefix_+"wlm",iter);
        writeSamples(&symbols[2+unkSymbolSize_],prefix_+"samp",iter);
        // TODO print step fst
        // TODO cumulate language models
        // TODO print cumulated language model
        // TODO print cumulated fst
        writeSymbols(prefix_+"sym",iter);
    }

    void iterateSamples(double annealLevel) {
        unsigned step = mySamples_.size()/100 + 1;
        cerr << "Running on "<<inputFsts_.size()<<" sequences (\".\"="<<step<<" sequences, \"!\"="<<step*10<<" sequences)"<<endl;
        time_t start = time(NULL);
        for(unsigned i = 0; i < mySamples_.size(); i++) {

#ifdef TimeDebug
        boost::posix_time::ptime time_now = boost::posix_time::microsec_clock::local_time();
    //cout<<"time_now "<<boost::posix_time::to_iso_string(time_now )<<endl;
#endif
            singleSample(mySamples_[i], annealLevel);
#ifdef TimeDebug
        boost::posix_time::ptime time_then = boost::posix_time::microsec_clock::local_time();
   // cout<<"time_then "<<boost::posix_time::to_iso_string(time_then )<<endl;
        singleSampletime =singleSampletime+( time_then - time_now);//
#endif
            if(i%step == step-1)
                cerr << (i/step%10 == 9 ? '!' : '.');
        }

        cerr << ' ' << (time(NULL)-start) << " seconds" << endl;
    }

    void singleSample(unsigned sentId, double annealLevel = 1) {
/*
#ifdef TimeDebug
        boost::posix_time::ptime time_nowa = boost::posix_time::microsec_clock::local_time();
        //cout<<"time_nowa "<<boost::posix_time::to_iso_string(time_nowa )<<endl;
#endif

        //cout<<"sentID"<<sentId<<endl;
#ifdef TimeDebug
        boost::posix_time::ptime time_now = boost::posix_time::microsec_clock::local_time();
#endif
        
        if(histories_[sentId].size())
            removeSample(sentId);
#ifdef TimeDebug
        boost::posix_time::ptime time_then = boost::posix_time::microsec_clock::local_time();
        removeSampletime =removeSampletime+( time_then - time_now);//
#endif
       
        // build
#ifdef TimeDebug
        time_now = boost::posix_time::microsec_clock::local_time();
#endif

        Fst<StdArc> * inputFst = createInputFst(sentId);
        ComposeFst<StdArc> ilFst(*inputFst, *lexFst_);

        PylmFst<WordId,CharId> pylmFst(*knownLm_, *unkLm_, unkSymbolSize_);
        ComposeFstOptions<StdArc, PM> copts(CacheOptions(),
                              new PM(ilFst, MATCH_NONE),
                              new PM(pylmFst, MATCH_INPUT,1));
        ComposeFst<StdArc> ilpFst(ilFst, pylmFst, copts);

        // prune
        VectorFst<StdArc> prunedFst;
        if(pruneThreshold_ != 0)
            Prune<StdArc>(ilpFst,&prunedFst,pruneThreshold_);
        else
            prunedFst = VectorFst<StdArc>(ilpFst);
        // check to make sure that pruning worked correctly
        if(prunedFst.NumStates() <= 1) {
            VectorFst<StdArc>(*inputFst).Write("inputFst.fst");
            VectorFst<StdArc>(ilFst).Write("ilFst.fst");
            VectorFst<StdArc>(ilpFst).Write("ilpFst.fst");
            VectorFst<StdArc>(pylmFst).Write("pylmFst.fst");
            THROW_ERROR("Pruned FST has one or fewer states\n");
        }

#ifdef TimeDebug
        time_then = boost::posix_time::microsec_clock::local_time();
        graphtime =graphtime+( time_then - time_now);//
#endif
        // sample
#ifdef TimeDebug
        time_now = boost::posix_time::microsec_clock::local_time();
#endif   
        VectorFst<StdArc> sampledFst;
        const vector<string>&  symbols = lexFst_->getSymbols();
        const string*  symbols1=&symbols[2+unkSymbolSize_];
        const string*  symbols4=&symbols[2];
        SampGen(prunedFst, sampledFst, symbols4,symbols1,unkSymbolSize_,1, annealLevel);
        // save and add
#ifdef TimeDebug
        time_then = boost::posix_time::microsec_clock::local_time();
        viterbetime =viterbetime+( time_then - time_now);//
#endif


#ifdef TimeDebug
        time_now = boost::posix_time::microsec_clock::local_time();
#endif
        histories_[sentId] = lexFst_->parseSample(sampledFst,annealLevel);
        //const vector<string>  symbols = lexFst_->getSymbols();
        //const vector<string>&  symbols2 = lexFst_->getSymbols();
#ifdef TimeDebug
        time_then = boost::posix_time::microsec_clock::local_time();
        parseSampletime =parseSampletime+( time_then - time_now);//
#endif
        //const string*  symbols3=&symbols2[2+unkSymbolSize_];

#ifdef TimeDebug
        time_now = boost::posix_time::microsec_clock::local_time();
#endif

        addSample(sentId);
#ifdef TimeDebug
        time_then = boost::posix_time::microsec_clock::local_time();
        traintime =traintime+( time_then - time_now);//
#endif
        if(!cacheInput_)
            delete inputFst;
        // calculate the likelihood
#ifdef TimeDebug
        time_now = boost::posix_time::microsec_clock::local_time();
#endif

        StdArc::StateId sid = sampledFst.Start();
        while(true) {
            ArcIterator< Fst<StdArc> > ai(sampledFst,sid);
            if(ai.Done()) break;
            latticeLikelihood_ += ai.Value().weight.Value();
            sid = ai.Value().nextstate;

        }

#ifdef TimeDebug
        time_then = boost::posix_time::microsec_clock::local_time();
        likelihoodime =traintime+( time_then - time_now);//
#endif
                
#ifdef TimeDebug
        boost::posix_time::ptime time_thena = boost::posix_time::microsec_clock::local_time();
        allSampletime =allSampletime+( time_thena - time_nowa);//
        //cout<<"time_thena "<<boost::posix_time::to_iso_string(time_thena )<<endl;
#endif 
*/    
    }
#ifdef TimeDebug
    void TimeShow()
    {
         cout<<"removeSampletime"<<boost::posix_time::to_iso_string(removeSampletime)<<endl;
         cout<<"graphtime"<<boost::posix_time::to_iso_string(graphtime)<<endl;
         cout<<"viterbetime"<<boost::posix_time::to_iso_string(viterbetime)<<endl;
         cout<<"parseSampletime"<<boost::posix_time::to_iso_string(parseSampletime)<<endl;
         cout<<"traintime"<<boost::posix_time::to_iso_string(traintime)<<endl;
         cout<<"sampleParameterstime"<<boost::posix_time::to_iso_string(sampleParameterstime)<<endl;
         cout<<"iterateSamplestime"<<boost::posix_time::to_iso_string(iterateSamplestime)<<endl;
         cout<<"likelihoodime"<<boost::posix_time::to_iso_string(likelihoodime)<<endl;
         cout<<"allSampletime"<<boost::posix_time::to_iso_string(allSampletime)<<endl;
         cout<<"singleSampletime"<<boost::posix_time::to_iso_string(singleSampletime)<<endl;
         cout<<"wholetime"<<boost::posix_time::to_iso_string(wholetime)<<endl;
         cout<<"composetime"<<boost::posix_time::to_iso_string(composetime)<<endl;

    }
#endif
    // remove a sample from the LMs
    void removeSample(unsigned sentId) { 
        knownLm_->removeCustomers(histories_[sentId]);
        const vector<int> & remPositions = knownLm_->getBasePositions();
        const vector< vector<CharId> > & knownWords = lexFst_->getWords();
        for(unsigned j = 0; j < remPositions.size(); j++)
            unkLm_->removeCustomers(knownWords[histories_[sentId][remPositions[j]]]);
    }

    // add the sample to the LMs
    void addSample(unsigned sentId) {
        const vector<WordId> & words = histories_[sentId];
        const vector< vector<CharId> > & knownWords = lexFst_->getWords();
        // get the word base probabilities
        vector<LMProb> knownBases(words.size(),0);
        for(unsigned j = 0; j < words.size(); j++) //后验                 //先验
            knownBases[j] = exp(unkLm_->calcSentence(knownWords[words[j]], unkBases_, false));
        // sample the LM and save the probability
        knownLikelihood_ -= knownLm_->calcSentence(words, knownBases, true);
        const vector<int> & addPositions = knownLm_->getBasePositions();
        for(unsigned j = 0; j < addPositions.size(); j++) 
            unkLikelihood_ -= unkLm_->calcSentence(knownWords[words[addPositions[j]]], unkBases_, true);
    }

    // create (or load) an FST representing the data
    Fst<StdArc> * createInputFst(unsigned sentId) {
        // cerr << "createInputFst("<<sentId<<") "<<cacheInput_<<", "<<inputFsts_.size()<<", "<<(int)inputFsts_[sentId]<<endl;
        if(cacheInput_ && inputFsts_.size() > sentId && inputFsts_[sentId])
            return inputFsts_[sentId];
        Fst<StdArc>* ret = NULL;
        WeightedMapper mapper(amScale_);
        Fst<StdArc> * nextFst = VectorFst<StdArc>::Read(inputFiles_[sentId]);
        cout<<"input"<<inputFiles_[sentId]<<endl;
        ret = new VectorFst<StdArc>;
        Map(*nextFst, (VectorFst<StdArc>*)ret, mapper);
        ArcSort((VectorFst<StdArc>*)ret,OLabelCompare<StdArc>());
        delete nextFst;
        if(cacheInput_) {
            if(inputFsts_.size() <= sentId) inputFsts_.resize(sentId+1,0);
            inputFsts_[sentId] = ret;
        }
        return ret;
    }

    // get the word base probabilities
    vector<LMProb> calculateWordBases() {
        const vector< vector<CharId> > & knownWords = lexFst_->getWords();
        vector<LMProb> bases(knownWords.size(),0);
        for(unsigned j = 0; j < knownWords.size(); j++) 
            bases[j] = exp(unkLm_->calcSentence(knownWords[j], unkBases_, false));
        return bases;
    }

    // write out the symbol file
    void writeSymbols(string fileName, int iter = -1) {
        if(!fileName.length())
            fileName = prefix_+"sym";
        if(iter >= 0) {
            ostringstream oss; oss << fileName << '.' << iter; 
            fileName = oss.str();
        }
        cerr << "  Writing symbols to "<<fileName<<endl;
        ofstream symOut(fileName.c_str());
        const vector<string> & words = lexFst_->getSymbols();
        for(unsigned i = 0; i < words.size(); i++)
            symOut << words[i] << "\t" << i << endl;
        symOut.close();
    }

    // write out the LM file
    template <class T>
    void writeLm(const PyLM<T> * lm, const string* symbols, const LMProb* bases, string fileName, int iter = -1) {
        if(!fileName.length())
            fileName = prefix_+"lm";
        if(iter >= 0) {
            ostringstream oss; oss << fileName << '.' << iter; 
            fileName = oss.str();
        }
        cerr << "  Writing LM to "<<fileName<<endl;
        ofstream lmOut(fileName.c_str());
        lm->print(symbols,bases,lmOut);
        lmOut.close();
    }

    template <class T>
    void computeLm(PyLM<T> * lm, const string* symbols, const LMProb& bases) {
        lm->compute(symbols,bases);
    }

    // write out the samples
    void writeSamples(const string* symbols, string fileName, int iter = -1) {
        if(!fileName.length())
            fileName = prefix_+"samp";
        if(iter >= 0) {
            ostringstream oss; oss << fileName << '.' << iter; 
            fileName = oss.str();
        }
        cerr << "  Writing samples to "<<fileName<<endl;
        ofstream sampOut(fileName.c_str());
        for(unsigned i = 0; i < histories_.size(); i++) {
            for(unsigned j = 0; j < histories_[i].size(); j++) {
                if(j) sampOut << " ";
                sampOut << symbols[histories_[i][j]].substr(1);
            }
            sampOut << endl;
        }
        sampOut.close();
    }

     
     bool mustSeg(const string& str)
     {
          for(unsigned i=0;i<stopWord.size();i++)
                if(stopWord[i]==str)
                      return true;
          return false;
     }
     void stringToFst(const string& input,string& ret)
     {
#ifdef TimeDebug
         boost::posix_time::ptime time_now = boost::posix_time::microsec_clock::local_time();
#endif
         string str;
         int state;
         listfst* fst=new listfst();
/*
         StdVectorFst * fst = new StdVectorFst;
         fst->AddState();
         fst->SetStart(0);
*/
         istringstream iss(input);
         //const vector<string>&  symbols = lexFst_->getSymbols();
         string buff;
         int i=0;
         for(state = 0; iss >> str; state++) {
               CharId lab = lexFst_->getId(str);
               //cout<<"str"<<str<<" "<<lab<<"state"<<state<<"sym"<<symbols[lab]<<endl;
               //cout<<lab<<"  ";
               if(lab==0||mustSeg(str))
               {
                   fst->SetFinal(state,0);
                   if(state>1)
                   {
                      if(i<=1)
                         ret+=fstToResult(fst)+str+" ";
                      else
                         ret+=fstToResult(fst)+" "+str+" ";
                   }
                   else
                   {
                       ret+=buff+" "+str+" ";
                   }
                   buff="";
                   delete fst;
                   //cout<<endl;
                   fst = new listfst ;//StdVectorFst;
                   //cout<<"new fst"<<endl;
                   //fst->AddState();
                   //fst->SetStart(0);
                   state = -1;
               }
               else
               {
                   fst->insert(lab);
                   //fst->AddState();
                   //fst->AddArc(state,StdArc(lab,lab,0,state+1));
                   buff+=str;
               }
               i++;
         }
         //cout<<endl;
         fst->SetFinal(state,0);
         if(state>1)
         {
               ret+=fstToResult(fst);
         }
         else
         {
               ret+=buff;
         }
#ifdef TimeDebug
        boost::posix_time::ptime time_then = boost::posix_time::microsec_clock::local_time();
        wholetime =wholetime+( time_then - time_now);//
#endif
         //ret+=fstToResult(fst);
         //return fst;
    } 


    string fstToResult(listfst* inputFst)//StdVectorFst *
    {
/*
       // Fst<StdArc> * inputFst = createInputFst(sentId);
#ifdef TimeDebug
        boost::posix_time::ptime time_now = boost::posix_time::microsec_clock::local_time();
#endif
        ComposeFst<StdArc> ilFst(*inputFst, *lexFst_);

        //PylmFst<WordId,CharId> pylmFst(*knownLm_, *unkLm_, unkSymbolSize_);
        ComposeFstOptions<StdArc, PM> copts(CacheOptions(),
                              new PM(ilFst, MATCH_NONE),
                              new PM(*pylmFst, MATCH_INPUT,1));
        ComposeFst<StdArc> ilpFst(ilFst, *pylmFst, copts);
#ifdef TimeDebug
        boost::posix_time::ptime time_then = boost::posix_time::microsec_clock::local_time();
        composetime =composetime+( time_then - time_now);//
#endif
#ifdef TimeDebug
        time_now = boost::posix_time::microsec_clock::local_time();
#endif
        // prune

        pruneThreshold_=100;
        VectorFst<StdArc> prunedFst;

        if(pruneThreshold_ != 0)
            Prune<StdArc>(ilpFst,&prunedFst,pruneThreshold_);
        else
            prunedFst = VectorFst<StdArc>(ilpFst);
#ifdef TimeDebug
        time_then = boost::posix_time::microsec_clock::local_time();
        graphtime =graphtime+( time_then - time_now);//
#endif
        // check to make sure that pruning worked correctly
        if(prunedFst.NumStates() <= 1) {
            VectorFst<StdArc>(*inputFst).Write("inputFst.fst");
            VectorFst<StdArc>(ilFst).Write("ilFst.fst");
            VectorFst<StdArc>(ilpFst).Write("ilpFst.fst");
            VectorFst<StdArc>(*pylmFst).Write("pylmFst.fst");
            THROW_ERROR("Pruned FST has one or fewer states\n");
        }

#ifdef TimeDebug
        //time_then = boost::posix_time::microsec_clock::local_time();
        //graphtime =graphtime+( time_then - time_now);//
#endif

#ifdef TimeDebug
        time_now = boost::posix_time::microsec_clock::local_time();
#endif   
        VectorFst<StdArc> sampledFst;
*/
        const vector<string>&  symbols = lexFst_->getSymbols();
        //const string*  symbols1=&symbols[2+unkSymbolSize_];
        //const string*  symbols4=&symbols[2];
        //SampGen(prunedFst, sampledFst, symbols4,symbols1,unkSymbolSize_,1, 100);
        // save and add
/*
#ifdef TimeDebug
        time_then = boost::posix_time::microsec_clock::local_time();
        viterbetime =viterbetime+( time_then - time_now);//
#endif
*/
        vector<WordId>  sentence= lexFst_->parseSample(inputFst,100);
        string ret;
        //const vector<string>&  symbols5 = lexFst_->getSymbols();
        const string*  symbols3= &symbols[2+unkSymbolSize_];
        for(unsigned j = 0; j < sentence.size(); j++) {
                if((j==1&&(symbols3[sentence[0]].substr(1).size()<=3||(symbols3[sentence[1]].substr(1).size()<=3&&!mustSeg(symbols3[sentence[1]].substr(1)))))||j==0)
                      ;
                else
                      ret+=" ";
                ret+= symbols3[sentence[j]].substr(1);             
                //cout<<sentence[j]<<"  "<<ret;
        }
        //cout<<endl;
        return ret;
    }

    string segString(string input) 
    {
        string ret;
        stringToFst(input,ret);
        return ret;
    }

///////////////////////
// utility functions //
///////////////////////
private:

};

}

#endif
